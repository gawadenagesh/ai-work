"""
Multi-document RAG demo (module: app)

This script demonstrates a simple Retrieval-Augmented Generation (RAG)
pipeline using LangChain components:

- Loads PDF files from a `data/` folder.
- Splits documents into chunks using `RecursiveCharacterTextSplitter`.
- Creates embeddings with `HuggingFaceEmbeddings` and stores them in Chroma.
- Uses `ChatOllama` to answer questions using retrieved context.

Usage:
- Install dependencies listed in `requirements.txt`.
- Ensure Ollama is running if using `ChatOllama`.
- Update the `folder_path` variable as needed, then run:

    python app.py

Notes:
- The vector DB persists in `./chroma_db`.
- If an answer is not present in the documents the assistant replies "I don't know."
"""

import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


# Load Pdf Data
def load_documents(folder_path: str):
    if not os.path.exists(folder_path):
        raise FileNotFoundError(f"Folder '{folder_path}' does not exist")

    documents = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".pdf"):
            file_path = os.path.join(folder_path, filename)
            print(f"üìÑ Loading: {filename}")
            try:
                loader = PyPDFLoader(file_path)
                documents.extend(loader.load())
            except Exception as e:
                print(f"‚ùå Error loading {filename}: {e}")
    return documents

# Chunking
def split_text(documents):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
    )
    chunks = splitter.split_documents(documents)
    print(f"‚úÇÔ∏è Created {len(chunks)} chunks")
    return chunks

# Embeddings
embedding_function = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Vector DB
def create_vector_store(chunks):
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embedding_function,
        persist_directory="./chroma_db",
        collection_name="rag_docs"
    )
    return vector_store

# LLM Model
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


def query_rag_system(query_text, vector_store):
    llm = ChatOllama(model="llama3.2:latest") # Make sure you have Ollama installed and running!

    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    prompt = ChatPromptTemplate.from_template(
        """
        You are a helpful assistant.
        Answer ONLY using the context below.
        If the answer is not present, say "I don't know."

        Context:
        {context}

        Question:
        {question}
        """
    )

    chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough(),
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain.invoke(query_text)

# main
def main():
    folder_path = "/Users/nageshgawade/Documents/www/ai-work/multi-doc-rag-app/data" # CHANGE THIS to your folder path

    if not os.path.exists("./chroma_db"):
        print("üì¶ No vector DB found. Creating one...")
        docs = load_documents(folder_path)
        chunks = split_text(docs)
        vector_store = create_vector_store(chunks)
        print("Vector database created")
    else:
        print("üì¶ Loading existing vector DB...")
        vector_store = Chroma(
            persist_directory="./chroma_db",
            embedding_function=embedding_function,
            collection_name="rag_docs"
        )

    while True:
        query = input("\n‚ùì Ask a question (or type 'exit'): ")
        if query.lower() == "exit":
            break

        print("ü§î Thinking...")
        answer = query_rag_system(query, vector_store)
        print("\nüß† Answer:\n", answer)

if __name__ == "__main__":
    main()